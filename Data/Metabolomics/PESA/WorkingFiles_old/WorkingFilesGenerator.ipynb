{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import Libraries\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "utilsPath = r'S:\\U_Proteomica\\UNIDAD\\software\\MacrosRafa\\data\\Metabolomics\\PESA_Integromics\\Data\\utils'\n",
    "if utilsPath not in sys.path:\n",
    "    sys.path.append(utilsPath)\n",
    "\n",
    "from myLog import myLog\n",
    "from PlotEDA import PlotEDA\n",
    "from PlotMV import PlotMV\n",
    "from PCA_UMAP import PCA_UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Constants\n",
    "#\n",
    "\n",
    "MVF_thr = 0.2\n",
    "MVO_thr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set constants\n",
    "#\n",
    "\n",
    "workingPath = r\"S:\\U_Proteomica\\UNIDAD\\software\\MacrosRafa\\data\\Metabolomics\\PESA_Integromics\\Data\\Metabolomics\\PESA\"\n",
    "c18p_path = os.path.join(workingPath, 'OriginalFiles', 'RBR_V1_C18_POS_LOESS_featureData.xlsx')\n",
    "c18n_path = os.path.join(workingPath, 'OriginalFiles', 'RBR_V1_C18_NEG_LOESS_featureData.xlsx')\n",
    "hpn_path = os.path.join(workingPath, 'OriginalFiles', 'RBR_V1_HILIC_POS-NEG_LOESS_featureData.xlsx')\n",
    "code2sn_path = os.path.join(workingPath, 'OriginalFiles', 'V1_Metab.xlsx')\n",
    "\n",
    "fileSummary = os.path.join(workingPath, 'WorkingFiles', 'Plots', 'SummaryPlots.html')\n",
    "fileCohort = os.path.join(workingPath, 'WorkingFiles', 'Plots', 'CohortPlots.html')\n",
    "fileGroup = os.path.join(workingPath, 'WorkingFiles', 'Plots', 'GroupPlots.html')\n",
    "filePCA = os.path.join(workingPath, 'WorkingFiles', 'Plots', 'PCAPlots.html')\n",
    "if os.path.exists(fileSummary): os.remove(fileSummary)\n",
    "if os.path.exists(fileCohort): os.remove(fileCohort)\n",
    "if os.path.exists(fileGroup): os.remove(fileGroup)\n",
    "if os.path.exists(filePCA): os.remove(filePCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Session\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Set logging\n",
    "#\n",
    "\n",
    "logw = myLog(os.path.join(workingPath,'WorkingFiles', 'info.log'))\n",
    "logw('Start Session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read tables\n",
    "#\n",
    "\n",
    "c18p = pd.read_excel(c18p_path)\n",
    "c18n = pd.read_excel(c18n_path)\n",
    "hpn = pd.read_excel(hpn_path)\n",
    "code2sn = pd.read_excel(code2sn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create f2info.tsv\n",
    "#\n",
    "\n",
    "# Add fid\n",
    "c18p['fid'] = [f\"C18P{i+1}\" for i in range(c18p.shape[0])]\n",
    "c18n['fid'] = [f\"C18N{i+1}\" for i in range(c18n.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "pcount = count(1,1)\n",
    "ncount = count(1,1)\n",
    "hpn['fid'] = [f\"HILP{next(pcount)}\" if i[1]=='P' else f\"HILN{next(ncount)}\" for i in hpn['Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2i_c18p = c18p.loc[:, ['fid', 'Name', 'Apex m/z', 'RT [min]']]\n",
    "f2i_c18p['column'] = 'C18'\n",
    "f2i_c18p['mode'] = 'POS'\n",
    "f2i_c18n = c18n.loc[:, ['fid', 'Name', 'Apex m/z', 'RT [min]']]\n",
    "f2i_c18n['column'] = 'C18'\n",
    "f2i_c18n['mode'] = 'NEG'\n",
    "f2i_hpn = hpn.loc[:, ['fid', 'Name', 'Apex m/z', 'RT [min]']]\n",
    "f2i_hpn['column'] = 'HILIC'\n",
    "f2i_hpn['mode'] = [\"POS\" if i[1]=='P' else \"NEG\" for i in hpn['Name']]\n",
    "\n",
    "f2i = pd.concat([\n",
    "    f2i_c18p,\n",
    "    f2i_c18n,\n",
    "    f2i_hpn\n",
    "])\n",
    "\n",
    "f2i.to_csv(os.path.join(workingPath, 'WorkingFiles', 'f2info.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maintain fid only\n",
    "\n",
    "[i.drop(['Name', 'Apex m/z', 'RT [min]'], inplace=True, axis=1) for i in [c18p, c18n, hpn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Add Seqn to HILIC file\n",
    "#\n",
    "\n",
    "hpnT = hpn.set_index('fid').T\n",
    "\n",
    "code2sn['code'] = [i.split('_')[1] for i in code2sn['Name']]\n",
    "code2sn = code2sn.set_index('code')\n",
    "\n",
    "hpnT = hpnT.set_index(code2sn.loc[[i.split('_')[1] for i in hpnT.index]]['Seqn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Xm.tsv\n",
    "\n",
    "c18pT = c18p.set_index('fid').T\n",
    "c18nT = c18n.set_index('fid').T\n",
    "\n",
    "xm = code2sn.loc[:, ['Seqn']].set_index('Seqn')\n",
    "\n",
    "for i in [c18pT, c18nT, hpnT]:\n",
    "    xm = xm.join(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm.to_csv(os.path.join(workingPath, 'WorkingFiles', 'Xm.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# filter by missing values, center&scale, impute missing values, check batch effect\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = pd.read_csv(r'S:\\U_Proteomica\\UNIDAD\\software\\MacrosRafa\\data\\Metabolomics\\PESA_Integromics\\Data\\Metadata\\PESA\\WorkingFiles\\main_metadata.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMV = PlotMV(xm, mdata, file=fileSummary)\n",
    "plotMV.plotSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of observations: 444\n",
      "Total number of features: 2611\n",
      "Total number of features with <20.0% of missing values(<88 of obs.): 2611\n"
     ]
    }
   ],
   "source": [
    "logw('')\n",
    "logw(f\"Total number of observations: {xm.shape[0]}\")\n",
    "logw(f\"Total number of features: {xm.shape[1]}\")\n",
    "logw(f\"Total number of features with <{MVF_thr*100}% of missing values(<{int(xm.shape[0]*MVF_thr)} of obs.): {((xm.isna().sum()/xm.shape[0])<=MVF_thr).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column\n",
      "C18      1557\n",
      "HILIC    1054\n",
      "dtype: int64\n",
      "\n",
      "column  mode\n",
      "C18     NEG      232\n",
      "        POS     1325\n",
      "HILIC   NEG      591\n",
      "        POS      463\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f2i.loc[:, ['column', 'mode']].groupby(['column']).size())\n",
    "print()\n",
    "print(f2i.loc[:, ['column', 'mode']].groupby(['column', 'mode']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Filter by missing values (although no feature will be removed)\n",
    "#\n",
    "\n",
    "xmf = xm.loc[:, xm.isna().sum()/xm.shape[0] <= MVF_thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Summary plots\n",
    "#\n",
    "\n",
    "plotEDA = PlotEDA(xmf, mdata, file=fileSummary)\n",
    "plotEDA.plotSummary(\n",
    "    r11=(0.75, 1.25), r12=(0,0.2), \n",
    "    r3=(-2,2), ry3=(0,1),\n",
    "    vl3=[0],\n",
    "    binsize=0.01,\n",
    "    titleLabel=f''\n",
    ")\n",
    "\n",
    "plotEDA = PlotEDA(xmf, mdata, file=fileCohort)\n",
    "plotEDA.plotByGroup('Cohort',vl1=[0],vl2=[1], r1=(0.9,1.1), r2=(-6,6), binsize=0.0005, plotN=True)\n",
    "plotEDA = PlotEDA(xmf, mdata, file=fileGroup)\n",
    "plotEDA.plotByGroup('Group',vl1=[0],vl2=[1], r1=(0.9,1.1), r2=(-6,6), binsize=0.0005, plotN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations with <10.0% of missing values: 383 / 444\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Filter observations by Missing values\n",
    "#\n",
    "\n",
    "plotMV = PlotMV(xmf, mdata, file=fileSummary)\n",
    "plotMV.plotSummaryObs()\n",
    "\n",
    "# Filter Observations by missing values\n",
    "\n",
    "xmf = xmf[xmf.isna().sum(axis=1)/xmf.shape[1]<MVO_thr]\n",
    "\n",
    "logw(f'Total number of observations with <{MVO_thr*100}% of missing values: {xmf.shape[0]} / {xm.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Standardize\n",
    "#\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "xmfn = pd.DataFrame(\n",
    "    StandardScaler().fit_transform(xmf),\n",
    "    columns=xmf.columns, index=xmf.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Imputation of missing values using KNN\n",
    "#\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "xmfnv = pd.DataFrame(\n",
    "    KNNImputer(n_neighbors=3).fit_transform(xmfn),\n",
    "    columns=xmfn.columns,\n",
    "    index=xmfn.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed missing values: 0/1000013 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Imputed missing values: {xmfn.isna().sum().sum()}/{xmfn.shape[0]*xmfn.shape[1]} ({round(xmfn.isna().sum().sum()/(xmfn.shape[0]*xmfn.shape[1])*100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: mgcv\n",
      "Loading required package: nlme\n",
      "This is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n",
      "Loading required package: genefilter\n",
      "Loading required package: BiocParallel\n",
      "Warning message:\n",
      "package 'BiocParallel' was built under R version 4.2.1 \n",
      "Found4batches\n",
      "Adjusting for12covariate(s) or covariate level(s)\n",
      "Standardizing Data across genes\n",
      "Fitting L/S model and finding priors\n",
      "Finding nonparametric adjustments\n",
      "Adjusting the Data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Correct Batch Effect\n",
    "# Comparamos batch effect correction con los dos tipos de escalado\n",
    "\n",
    "# from combat import combat\n",
    "# from scipy.stats import kruskal, median_test\n",
    "\n",
    "# xmfnvb = combat(\n",
    "#     data=xmfnv.T,\n",
    "#     batch=mdata.set_index('Seqn').loc[xmfnv.index, 'Cohort']\n",
    "# ).T\n",
    "\n",
    "from myComBat import myComBat\n",
    "\n",
    "catVars = ['Group', 'Smoke']\n",
    "conVars = ['Calcium_Score', 'HDL', 'LDL', 'Total_Cholesterol','Ox-LDL','Lipoprotein(a)','CRP', 'Plaque_thickness']\n",
    "xmfnvb = myComBat(xmfnv, mdata, 'Cohort', catVars, conVars, Rpath=os.path.join(workingPath, 'WorkingFiles', 'myRData'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEDA = PlotEDA(xmfnvb, mdata, file=fileCohort)\n",
    "plotEDA.plotByGroup('Cohort',vl1=[0],vl2=[1], r1=(-6,6),binsize=0.0005, plotN=False)\n",
    "plotEDA = PlotEDA(xmfnvb, mdata, file=fileGroup)\n",
    "plotEDA.plotByGroup('Group',vl1=[0],vl2=[1], r1=(-6,6),binsize=0.0005, plotN=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotEDA = PlotEDA(xmfnvb, mdata)#, file=fileCohort)\n",
    "# plotEDA.plotByGroup('Cohort',vl1=[0],vl2=[1], r1=(-6,6), r2=(-6,6), binsize=0.0005, plotN=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=57.44098227377981, pvalue=2.0690579817047877e-12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotEDA._kruskal(xmfnvb, 'Cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate \n",
    "#\n",
    "\n",
    "xmfnvb.to_csv(os.path.join(workingPath, 'WorkingFiles', 'Xm_norm.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dimensionality Reduction\n",
    "#\n",
    "\n",
    "pcaumap = PCA_UMAP(xmfnv, mdata, file=filePCA)\n",
    "pcaumap.plotReduction('Cohort', pcacomp=[0,1])\n",
    "pcaumap.plotReduction('Group', pcacomp=[0,1])\n",
    "\n",
    "pcaumap = PCA_UMAP(xmfnvb, mdata, file=filePCA)\n",
    "pcaumap.plotReduction('Cohort', pcacomp=[0,1], titleLabel='- Batch Corrected')\n",
    "pcaumap.plotReduction('Group', pcacomp=[0,1], titleLabel='- Batch Corrected')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "299fabdee10379681b2207a83aa9f93c313ee5d5504e286e44b436bd7e45d8f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
